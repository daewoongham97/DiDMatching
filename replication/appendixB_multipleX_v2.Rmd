---
title: "Simulation to assess varying coefficients and guideline"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
library( tidyverse )
library(ggplot2); library(latex2exp); library(gridExtra); library(ggpubr)

theme_set( theme_minimal() )
knitr::opts_chunk$set(fig.width = 6,
                      fig.height = 3,
                      out.width = "80%", 
                      fig.align = "center",
                      warning = FALSE )
options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )


source( here::here( "replication/sim_functions.R" ) )
source( here::here( "oracle_bias_calculators.R" ) )
source( here::here( "DiD_matching_func.R" ) )

```


This runs a series of small simulations where we examine varying beta coefficients over time. In these simulations, we generate data and estimate the guidelines based on that data, and then compare these estimated recommendations to the oracle truth (using our general theorem) to see how our guideline works when it is technically misspecified. 

In particular, this document produces Figure 4 and Table 2 in Appendix B. 



# Run simulation across range of sigma_pre

Our initial simulation has varying coefficients for both $X$ and $Z$, along with $\theta$. The misspecification gives a reduced recommendation to match as shown on the figure to the right.

```{r}
# Number of simulation replicates per scenario
K = 2 # 25 #1000

cor_Xtheta = c( 0.3, 0.6 )

sigma_pre_tests = seq( 0.3, 1.5, by=0.10 ) 
names(sigma_pre_tests) = sigma_pre_tests 

sim_res_main <- map_df( sigma_pre_tests, 
                   ~ run_scenario( sigma_pre = .,
                                   beta_theta_0 = c( 0.5, 1.0 ),
                                   beta_theta_1 = 1.5,
                                   beta_x_0 = c( 0.6, 1.1),
                                   beta_x_1 = 1.3,
                                   beta_z_0 = c( 0.3, 0.7 ),
                                   beta_z_1 = 1.0,
                                   cor_XZ = 0.5,
                                   cor_Xtheta = cor_Xtheta,
                                   K = K ),
                   .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )

saveRDS(sim_res_main, file = "results/main_results.rds")
```

Our table shows, for different residual variation, the proportion of the trials that say "match!", the average estimated reduction in bias, the standard deviation of the estimates across simulation (which is the true SE), and whether the oracle says to match and how much bias would be reduced.  The last column is the $R^2$ for a regression of outcome onto the two observed covariates to get a sense of how much variation is explained by what we can match on.

```{r, echo=FALSE}
sim_res_main %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )
```

The plot is as discussed in the appendix:

```{r, echo=FALSE}
plt <- make_result_plot( sim_res_main )
plt
```


\newpage

## Independent covariates

This is our initial simulation, above, except all three covariates are now independent from each other.  The independence does not help us?

```{r}
sim_res_indep <- map_df( sigma_pre_tests, 
                    ~ run_scenario( sigma_pre = .,
                                    beta_theta_0 = c( 0.5, 1.0 ),
                                    beta_theta_1 = 1.5,
                                    beta_x_0 = c( 0.6, 1.1),
                                    beta_x_1 = 1.3,
                                    beta_z_0 = c( 0.3, 0.7 ),
                                    beta_z_1 = 1.0,
                                    cor_XZ = 0.0,
                                    cor_Xtheta = c(0,0),
                                    K = K ),
                    .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )
```

```{r, echo=FALSE}
saveRDS(sim_res_indep, file = "results/sim_res_indep.rds")

sim_res_indep %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )

plt <- make_result_plot( sim_res_indep )
plt
```


\newpage

## All is parallel

If we have parallel trends for all three covariates (two observed, one latent) then our guideline works as expected.

Also note that due to large sample size our match recommendation is very precisely estimated because our bias reduction is also very precisely estimated (see the SE column).

```{r}
sim_res_correct <- map_df( sigma_pre_tests, 
                    ~ run_scenario( sigma_pre = .,
                                    K = K,
                                    beta_theta_0 = c( 0.75, 0.75 ),
                                    beta_theta_1 = 1.5,
                                    beta_x_0 = c( 0.85, 0.85),
                                    beta_x_1 = 1.3,
                                    beta_z_0 = c( 0.5, 0.5 ),
                                    beta_z_1 = 1.0,
                                    cor_Xtheta = cor_Xtheta,
                                    cor_XZ = 0.5 ),                                                        .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )
```

```{r, echo=FALSE}
saveRDS(sim_res_correct, file = "results/sim_res_correct.rds")
sim_res_correct = readRDS( "results/sim_res_correct.rds" )

sim_res_correct %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )

plt <- make_result_plot( sim_res_correct )
plt
```


\newpage

## theta parallel, covariates not

Here we have X and Z correlated, but theta is parallel.

```{r}

sim_res3 <- map_df( sigma_pre_tests, 
                    ~ run_scenario( sigma_pre = .,
                                    K = K,
                                    beta_theta_0 = c( 0.75, 0.75 ),
                                    beta_theta_1 = 1.5,
                                    beta_x_0 = c( 0.6, 1.1),
                                    beta_x_1 = 1.3,
                                    beta_z_0 = c( 0.3, 0.7 ),
                                    beta_z_1 = 1.0,
                                    cor_Xtheta = cor_Xtheta,
                                    cor_XZ = 0.5 ),
                    .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )
```

```{r, echo=FALSE}
saveRDS(sim_res_theta_par, file = "results/sim_res_theta_par.rds")
sim_res_theta_par = readRDS("results/sim_res_theta_par.rds" )
sim_res_theta_par %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )

plt <- make_result_plot( sim_res_theta_par )
plt
```



\newpage

## small sample size

From our original simulation, if we reduce sample size, estimation error should flatten our curve.  It seems like estimation error is very small, which is surprising given all the residualization?


```{r}

sim_resSS <- map_df( sigma_pre_tests, 
                    ~ run_scenario( sigma_pre = .,
                                    beta_theta_0 = c( 0.5, 1.0 ),
                                    beta_theta_1 = 1.5,
                                    beta_x_0 = c( 0.6, 1.1 ),
                                    beta_x_1 = 1.3,
                                    beta_z_0 = c( 0.3, 0.7 ),
                                    beta_z_1 = 1.0,
                                    cor_Xtheta = cor_Xtheta,
                                    cor_XZ = 0.5,
                                    N = 3000 ),
                    .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )
```

```{r, echo=FALSE}
saveRDS(sim_resSS, file = "results/sim_resSS.rds" )
sim_resSS = readRDS( "results/sim_resSS.rds" )
sim_resSS %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )

plt <- make_result_plot( sim_resSS )
plt
```

The uncertainty in estimation of the reduction does go up quite a bit:

```{r}
sim_resSS$SE_tau_xy / sim_res_main$SE_tau_xy
```


\newpage

# More time periods with less predictive covariates

If we make covariates less predictive, but have more pre-treatment time periods? (We have also increased variation in theta to cause more trouble.)

```{r}

sigmas_larger = sigma_pre_tests * 2
names(sigmas_larger) = sigmas_larger
sim_res_T4 <- map_df( sigmas_larger, 
                    ~ run_scenario( sigma_pre = .,
                                    beta_theta_0 = c( 0.0, 0.4, 0.8, 1.2 ),
                                    beta_theta_1 = 1.6,
                                    beta_x_0 = c( 0.6, 1.1, 1.1, 0.6 ),
                                    beta_x_1 = 1.3,
                                    beta_z_0 = c( 0.7, 0.7, 0.3, 0.3 ),
                                    beta_z_1 = 1.0,
                                    cor_Xtheta = cor_Xtheta,
                                    cor_XZ = 0.5,
                                    K = K ),
                    .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )
```

```{r, echo=FALSE}
saveRDS(sim_res_T4, file = "results/sim_res_T4.rds")

sim_res_T4 %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )


plt <- make_result_plot( sim_res_T4 )
plt
```



\newpage

## narrow theta assumption only

If all we get is parallel theta in the final two periods, but theta is not parallel before that?

```{r}

sim_res_T4_narrow <- map_df( sigmas_larger, 
                    ~ run_scenario( sigma_pre = .,
                                    beta_theta_0 = c( 0.0, 0.4, 1.0, 1.0 ),
                                    beta_theta_1 = 1.6,
                                    beta_x_0 = c( 0.6, 1.1, 1.1, 0.6 ),
                                    beta_x_1 = 1.3,
                                    beta_z_0 = c( 0.7, 0.7, 0.3, 0.3 ),
                                    beta_z_1 = 1.0,
                                    cor_Xtheta = cor_Xtheta,
                                    cor_XZ = 0.5,
                                    K = K ),
                    .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )
```

```{r, echo=FALSE}
saveRDS(sim_res_T4_narrow, file = "results/sim_res_T4_narrow.rds")

sim_res_T4_narrow %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )

plt <- make_result_plot( sim_res_T4_narrow )
plt
```


\newpage

## All covariates independent, four time points

Here we have our theta stable in the final two time periods, and no correlation between any of our three. covariates.

```{r}

sim_res_T4_indep <- map_df( sigmas_larger, 
                    ~ run_scenario( sigma_pre = .,
                                    beta_theta_0 = c( 0.0, 0.4, 1.0, 1.0 ),
                                    beta_theta_1 = 1.6,
                                    beta_x_0 = c( 0.6, 1.1, 1.1, 0.6 ),
                                    beta_x_1 = 1.3,
                                    beta_z_0 = c( 0.7, 0.7, 0.3, 0.3 ),
                                    beta_z_1 = 1.0,
                                    cor_XZ = 0,
                                    cor_Xtheta = c( 0, 0 ),
                                    K = K ),
                    .id = "sigma_pre" ) %>%
    mutate( sigma_pre = as.numeric(sigma_pre) )
```

```{r, echo=FALSE}
saveRDS(sim_res_T4_indep, file = "results/sim_res_T4_indep.rds")
sim_res_T4_indep = readRDS( "results/sim_res_T4_indep.rds" )

sim_res_T4_indep %>% 
    dplyr::select( sigma_pre, per_match, a_tau_xy, SE_tau_xy, match_XY, reduce_XY, R2 ) %>%
    knitr::kable( digits = 3 )

plt <- make_result_plot( sim_res_T4_indep )
plt
```


# All biases across scenarios

These plots aggregate the above to compare trends and sizes of biases, etc.

```{r, echo=FALSE}

all_res = bind_rows( main = sim_res_main,
                     independent = sim_res_indep,
                     all_parallel = sim_res_correct,
                     theta_parallel = sim_res_theta_par,
                     four_time = sim_res_T4,
                     four_time_narrow = sim_res_T4_narrow,
                     four_time_indep = sim_res_T4_indep,
                     small_sample_size = sim_resSS, .id = "scenario" )

arL <- all_res %>% 
    pivot_longer( cols = c(a_tau_xy, reduce_XY ),
                  names_to = "metric", values_to = "value" )
ggplot( arL, aes( sigma_pre, value, col=scenario ) ) +
    facet_wrap( ~ metric ) +
    geom_line() +
    geom_hline( yintercept = 0 )
```

```{r, echo=FALSE, fig.height=6}
ggplot( all_res, aes( sigma_pre, a_tau_xy, col="estimate" ) ) +
    facet_wrap( ~ scenario ) +
    geom_line() +
    geom_line( aes( y = reduce_XY, col="oracle")) +
    geom_hline( yintercept = 0 )

all_res$match_XY = as.numeric( all_res$match_XY )
ggplot( all_res, aes( sigma_pre, per_match ) ) +
    facet_wrap( ~ scenario ) +
    geom_line( aes( y = match_XY ), col="grey" ) +
    geom_line() +
    geom_hline( yintercept = 0 )

```

